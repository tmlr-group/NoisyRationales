# noise test
model: gpt-3.5-turbo-0613 # gpt-3.5-turbo-0613 or gemini-pro or mixtral or  llama-2-70b
dataset:  math # math and symbolic and commonsense 
start_num: 0
test_num: 300
batch_size: 5

# subtask
## math
math:
  subtask: base-9

## symbolic
symbolic:
  subtask: longer # equal or longer


# use subfolder_suffix
# subfolder_suffix_path: 4_shot_ablation

# use_processed_dataset or use raw dataset
use_processed_dataset: True

## when use_processed_dataset is True
processed_dataset_options:
  ## processed dataset path or one of ["default-zeroshot"ï¼Œ "default-clean", "default-(irrelevant|inaccurate)-(easy|medium|hard)-(fixed|random)"]
  processed_dataset_path: default-irrelevant-easy
  n_shots: 3
  using_subset: False
  # processed_dataset_path: default-zeroshot
  # processed_dataset_path: default-clean

# when use_processed_datset is False
raw_dataset_options:
  ## in-context 
  if_in_context: True
  n_shots: 0 # self-made excellent shots
  # n_weak_shots: 0
  ## noise
  if_noise: True
  n_noisy_shots: 10
  noise_type: inaccurate # irrelevant or inaccurate
  noise_ratio: 0.8 # 0.3, 0.5 or 0.8
  noise_distribution: fixed #fixed or random

# ICL format
prefix_context: False ## prefix

# method
method: basemodel # CD-CoT or basemodel,  smoothllm, selfdenoise, selfpolish, contrastivecot, ISC, SCO, BT


## basemodel, smoothllm, selfdenoise, selfpolish, contrastivecot, ISC
temperature_reason: 1
n_reason: 5

## CD-CoT
use_logged_rephrased_result: False
n_rephrase: 5
temperature_rephrase: 1
topp_rephrase: 1
m_select: 2 # 1 or 2 or 3 or 5
use_logged_ICL_result: False
use_clean_shot: True
# use_noisy_shot: False
c_reason: [3,2] # [5] or [3,2] or [2,2,1] or [1,1,1,1,1]
temp_reason: 1
topp_reason: 1

# model
## gpt
gpt:
  api: openai  # openai
